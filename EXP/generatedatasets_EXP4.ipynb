{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e8e0a3",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a679a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images and dataset for task: framed\n",
      "Generating images and dataset for task: unframed\n",
      "Training dataset saved as 'train_dataset.json' in 'finetuning-EXP4-5000-10epochs-test/json'\n",
      "Validation dataset saved as 'val_dataset.json' in 'finetuning-EXP4-5000-10epochs-test/json'\n",
      "Test dataset saved as 'test_dataset.json' in 'finetuning-EXP4-5000-10epochs-test/json'\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage.draw\n",
    "\n",
    "# Custom module import\n",
    "sys.path.append(\"../\")  # Add parent directory to Python path\n",
    "import LLMP as L\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "\n",
    "# Main output directory\n",
    "main_output_dir = \"finetuning-EXP4-5000-10epochs-test\"  # Directory to store results\n",
    "# Number of images to generate for each task\n",
    "num_images_per_task = 5000  # Total images per task\n",
    "\n",
    "# Subdirectories for images and JSON files\n",
    "image_output_dir = os.path.join(main_output_dir, \"images\")  # Directory for saving images\n",
    "json_output_dir = os.path.join(main_output_dir, \"json\")  # Directory for saving JSON files\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(image_output_dir, exist_ok=True)  # Ensure image directory exists\n",
    "os.makedirs(json_output_dir, exist_ok=True)  # Ensure JSON directory exists\n",
    "\n",
    "# Define tasks with associated questions\n",
    "tasks = {\n",
    "    \"framed\": \"Estimate the lengths of the two bars with framing. Both lengths should fall between 49 and 60 pixels. No explanation. Format of the answer [xx, xx]\",\n",
    "    \"unframed\": \"Estimate the lengths of the two bars without framing. Both lengths should fall between 49 and 60 pixels. No explanation. Format of the answer [xx, xx]\"\n",
    "}\n",
    "\n",
    "# List to store all data from all tasks\n",
    "combined_dataset = []  # Placeholder for the entire dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "# Initialize counters for each dataset type\n",
    "train_counter = 0  # Counter for training set images\n",
    "val_counter = 0  # Counter for validation set images\n",
    "test_counter = 0  # Counter for test set images\n",
    "\n",
    "# Define the target number of images for each dataset\n",
    "train_target = 5000  # Number of images for training\n",
    "val_target = 1000  # Number of images for validation\n",
    "test_target = 500  # Number of images for testing\n",
    "\n",
    "# Combined dataset split lists\n",
    "combined_dataset_training = []  # List to store training dataset entries\n",
    "combined_dataset_validation = []  # List to store validation dataset entries\n",
    "combined_dataset_testing = []  # List to store test dataset entries\n",
    "\n",
    "# Separate lists to track unique labels in each dataset\n",
    "train_labels = []  # Labels in the training set\n",
    "val_labels = []  # Labels in the validation set\n",
    "test_labels = []  # Labels in the test set\n",
    "######################################\n",
    "\n",
    "# Loop through each task\n",
    "for task, question in tasks.items():\n",
    "    print(f\"Generating images and dataset for task: {task}\")  # Inform user about the current task\n",
    "    \n",
    "    while train_counter < train_target or val_counter < val_target or test_counter < test_target:\n",
    "\n",
    "        image_array, label = L.GPImage.figure12(task)  # Generate image and label using the custom module\n",
    "        pot = np.random.choice(3)  # Randomly assign potential dataset\n",
    "\n",
    "        # Ensure global label separation between datasets\n",
    "        if label in train_labels or label == min(train_labels, default=None) or label == max(train_labels, default=None):\n",
    "            pot = 0  # Training set priority\n",
    "        elif label in val_labels or label == min(val_labels, default=None) or label == max(val_labels, default=None):\n",
    "            pot = 1  # Validation set priority\n",
    "        elif label in test_labels or label == min(test_labels, default=None) or label == max(test_labels, default=None):\n",
    "            pot = 2  # Test set priority\n",
    "\n",
    "        # Training dataset conditional checks\n",
    "        if pot == 0 and train_counter < train_target:\n",
    "            if label not in train_labels:  # Avoid duplicate labels\n",
    "                train_labels.append(label)\n",
    "            \n",
    "            image_array = image_array.astype(np.float32)  # Convert to float32 to add noise\n",
    "\n",
    "            # Add noise only to the background (value == 0)\n",
    "            noise_mask = (image_array == 0)  # Identify background pixels\n",
    "            noise = np.random.uniform(0, 0.05, image_array.shape)  # Generate random noise\n",
    "            image_array[noise_mask] += noise[noise_mask]  # Apply noise to background only\n",
    "\n",
    "            label = [round(float(val), 2) for val in label]  # Round label values to 2 decimals\n",
    "\n",
    "            image_array_uint8 = (image_array * 255).astype(np.uint8)  # Scale image values to 0-255\n",
    "            pil_image = Image.fromarray(image_array_uint8)  # Convert to PIL image\n",
    "            unique_id = str(uuid.uuid4())  # Generate a unique identifier for the image\n",
    "            image_filename = os.path.join(image_output_dir, f\"{unique_id}.jpg\")  # Set the image filename\n",
    "            pil_image.save(image_filename)  # Save the image\n",
    "\n",
    "            json_entry = {\n",
    "                'id': unique_id,\n",
    "                'image': f\"{unique_id}.jpg\",\n",
    "                'question': question,\n",
    "                'value': label\n",
    "            }\n",
    "\n",
    "            combined_dataset_training.append(json_entry)  # Add entry to the training dataset\n",
    "            train_counter += 1  # Increment training counter\n",
    "\n",
    "        # Validation dataset conditional checks\n",
    "        elif pot == 1 and val_counter < val_target:\n",
    "            if label not in val_labels:  # Avoid duplicate labels\n",
    "                val_labels.append(label)\n",
    "\n",
    "            image_array = image_array.astype(np.float32)  # Convert to float32 to add noise\n",
    "\n",
    "            # Add noise only to the background (value == 0)\n",
    "            noise_mask = (image_array == 0)  # Identify background pixels\n",
    "            noise = np.random.uniform(0, 0.05, image_array.shape)  # Generate random noise\n",
    "            image_array[noise_mask] += noise[noise_mask]  # Apply noise to background only\n",
    "\n",
    "            label = [round(float(val), 2) for val in label]  # Round label values to 2 decimals\n",
    "\n",
    "            image_array_uint8 = (image_array * 255).astype(np.uint8)  # Scale image values to 0-255\n",
    "            pil_image = Image.fromarray(image_array_uint8)  # Convert to PIL image\n",
    "            unique_id = str(uuid.uuid4())  # Generate a unique identifier for the image\n",
    "            image_filename = os.path.join(image_output_dir, f\"{unique_id}.jpg\")  # Set the image filename\n",
    "            pil_image.save(image_filename)  # Save the image\n",
    "\n",
    "            json_entry = {\n",
    "                'id': unique_id,\n",
    "                'image': f\"{unique_id}.jpg\",\n",
    "                'question': question,\n",
    "                'value': label\n",
    "            }\n",
    "\n",
    "            combined_dataset_validation.append(json_entry)  # Add entry to the validation dataset\n",
    "            val_counter += 1  # Increment validation counter\n",
    "\n",
    "        # Test dataset conditional checks\n",
    "        elif pot == 2 and test_counter < test_target:\n",
    "            if label not in test_labels:  # Avoid duplicate labels\n",
    "                test_labels.append(label)\n",
    "\n",
    "            image_array = image_array.astype(np.float32)  # Convert to float32 to add noise\n",
    "\n",
    "            # Add noise only to the background (value == 0)\n",
    "            noise_mask = (image_array == 0)  # Identify background pixels\n",
    "            noise = np.random.uniform(0, 0.05, image_array.shape)  # Generate random noise\n",
    "            image_array[noise_mask] += noise[noise_mask]  # Apply noise to background only\n",
    "\n",
    "            label = [round(float(val), 2) for val in label]  # Round label values to 2 decimals\n",
    "\n",
    "            image_array_uint8 = (image_array * 255).astype(np.uint8)  # Scale image values to 0-255\n",
    "            pil_image = Image.fromarray(image_array_uint8)  # Convert to PIL image\n",
    "            unique_id = str(uuid.uuid4())  # Generate a unique identifier for the image\n",
    "            image_filename = os.path.join(image_output_dir, f\"{unique_id}.jpg\")  # Set the image filename\n",
    "            pil_image.save(image_filename)  # Save the image\n",
    "\n",
    "            json_entry = {\n",
    "                'id': unique_id,\n",
    "                'image': f\"{unique_id}.jpg\",\n",
    "                'question': question,\n",
    "                'value': label\n",
    "            }\n",
    "\n",
    "            combined_dataset_testing.append(json_entry)  # Add entry to the test dataset\n",
    "            test_counter += 1  # Increment test counter\n",
    "\n",
    "# Save the combined dataset as separate JSON files\n",
    "combined_json_training_filename = \"train_dataset.json\"\n",
    "combined_json_training_filepath = os.path.join(json_output_dir, combined_json_training_filename)\n",
    "with open(combined_json_training_filepath, 'w') as json_file:\n",
    "    json.dump(combined_dataset_training, json_file, indent=4)  # Save training dataset to JSON\n",
    "print(f\"Training dataset saved as '{combined_json_training_filename}' in '{json_output_dir}'\")\n",
    "\n",
    "\n",
    "combined_json_validation_filename = \"val_dataset.json\"\n",
    "combined_json_validation_filepath = os.path.join(json_output_dir, combined_json_validation_filename)\n",
    "with open(combined_json_validation_filepath, 'w') as json_file:\n",
    "    json.dump(combined_dataset_validation, json_file, indent=4)  # Save validation dataset to JSON\n",
    "print(f\"Validation dataset saved as '{combined_json_validation_filename}' in '{json_output_dir}'\")\n",
    "\n",
    "combined_json_testing_filename = \"test_dataset.json\"\n",
    "combined_json_testing_filepath = os.path.join(json_output_dir, combined_json_testing_filename)\n",
    "with open(combined_json_testing_filepath, 'w') as json_file:\n",
    "    json.dump(combined_dataset_testing, json_file, indent=4)  # Save test dataset to JSON\n",
    "print(f\"Test dataset saved as '{combined_json_testing_filename}' in '{json_output_dir}'\")\n",
    "\n",
    "\n",
    "i am curious why testing has more unique labels from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff58abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d6c950f",
   "metadata": {},
   "source": [
    "### Check overlap labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4c90435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Dataset      | Total Unique Labels | Unique Labels\n",
      "|--------------|---------------------|---------------\n",
      "| Training     | 34                  | [(49.0, 52.0), (49.0, 53.0), (49.0, 57.0), (49.0, 59.0), (49.0, 60.0), (50.0, 55.0), (50.0, 56.0), (51.0, 52.0), (52.0, 51.0), (52.0, 55.0), (53.0, 49.0), (53.0, 50.0), (53.0, 51.0), (53.0, 56.0), (53.0, 58.0), (54.0, 49.0), (54.0, 50.0), (54.0, 57.0), (55.0, 51.0), (55.0, 54.0), (56.0, 51.0), (56.0, 54.0), (56.0, 55.0), (57.0, 49.0), (57.0, 51.0), (57.0, 54.0), (57.0, 55.0), (57.0, 58.0), (58.0, 59.0), (59.0, 55.0), (59.0, 57.0), (60.0, 52.0), (60.0, 56.0), (60.0, 59.0)]\n",
      "| Validation   | 44                  | [(49.0, 51.0), (49.0, 55.0), (50.0, 51.0), (50.0, 53.0), (50.0, 54.0), (50.0, 59.0), (50.0, 60.0), (51.0, 49.0), (51.0, 50.0), (51.0, 53.0), (51.0, 54.0), (51.0, 55.0), (51.0, 56.0), (52.0, 50.0), (52.0, 53.0), (52.0, 54.0), (52.0, 56.0), (52.0, 59.0), (52.0, 60.0), (53.0, 52.0), (53.0, 59.0), (53.0, 60.0), (54.0, 51.0), (54.0, 53.0), (54.0, 55.0), (54.0, 56.0), (54.0, 58.0), (55.0, 49.0), (55.0, 50.0), (55.0, 52.0), (55.0, 58.0), (56.0, 50.0), (57.0, 56.0), (58.0, 49.0), (58.0, 50.0), (58.0, 56.0), (59.0, 50.0), (59.0, 52.0), (59.0, 54.0), (59.0, 56.0), (60.0, 49.0), (60.0, 50.0), (60.0, 54.0), (60.0, 55.0)]\n",
      "| Testing      | 54                  | [(49.0, 50.0), (49.0, 54.0), (49.0, 56.0), (49.0, 58.0), (50.0, 49.0), (50.0, 52.0), (50.0, 57.0), (50.0, 58.0), (51.0, 57.0), (51.0, 58.0), (51.0, 59.0), (51.0, 60.0), (52.0, 49.0), (52.0, 57.0), (52.0, 58.0), (53.0, 54.0), (53.0, 55.0), (53.0, 57.0), (54.0, 52.0), (54.0, 59.0), (54.0, 60.0), (55.0, 53.0), (55.0, 56.0), (55.0, 57.0), (55.0, 59.0), (55.0, 60.0), (56.0, 49.0), (56.0, 52.0), (56.0, 53.0), (56.0, 57.0), (56.0, 58.0), (56.0, 59.0), (56.0, 60.0), (57.0, 50.0), (57.0, 52.0), (57.0, 53.0), (57.0, 59.0), (57.0, 60.0), (58.0, 51.0), (58.0, 52.0), (58.0, 53.0), (58.0, 54.0), (58.0, 55.0), (58.0, 57.0), (58.0, 60.0), (59.0, 49.0), (59.0, 51.0), (59.0, 53.0), (59.0, 58.0), (59.0, 60.0), (60.0, 51.0), (60.0, 53.0), (60.0, 57.0), (60.0, 58.0)]\n",
      "\n",
      "Overlap Information:\n",
      "- No overlap between Training and Validation.\n",
      "- No overlap between Training and Testing.\n",
      "- No overlap between Validation and Testing.\n"
     ]
    }
   ],
   "source": [
    "# Script to count unique labels across datasets and check overlap\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load datasets\n",
    "json_output_dir = \"finetuning-EXP4-5000-10epochs-test/json\"\n",
    "\n",
    "train_file = os.path.join(json_output_dir, \"train_dataset.json\")\n",
    "val_file = os.path.join(json_output_dir, \"val_dataset.json\")\n",
    "test_file = os.path.join(json_output_dir, \"test_dataset.json\")\n",
    "\n",
    "with open(train_file, 'r') as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "with open(val_file, 'r') as f:\n",
    "    val_dataset = json.load(f)\n",
    "\n",
    "with open(test_file, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# Function to count unique labels in a dataset\n",
    "def count_unique_labels(dataset):\n",
    "    labels = [tuple(entry['value']) for entry in dataset]\n",
    "    unique_labels = set(labels)\n",
    "    return unique_labels\n",
    "\n",
    "# Count unique labels for each dataset\n",
    "train_unique_labels = count_unique_labels(train_dataset)\n",
    "val_unique_labels = count_unique_labels(val_dataset)\n",
    "test_unique_labels = count_unique_labels(test_dataset)\n",
    "\n",
    "# Check for overlaps across datasets\n",
    "train_val_overlap = train_unique_labels & val_unique_labels\n",
    "train_test_overlap = train_unique_labels & test_unique_labels\n",
    "val_test_overlap = val_unique_labels & test_unique_labels\n",
    "\n",
    "# Print unique labels for each dataset\n",
    "print(\"| Dataset      | Total Unique Labels | Unique Labels\")\n",
    "print(\"|--------------|---------------------|---------------\")\n",
    "print(f\"| Training     | {len(train_unique_labels):<19} | {sorted(train_unique_labels)}\")\n",
    "print(f\"| Validation   | {len(val_unique_labels):<19} | {sorted(val_unique_labels)}\")\n",
    "print(f\"| Testing      | {len(test_unique_labels):<19} | {sorted(test_unique_labels)}\")\n",
    "\n",
    "# Print overlap information\n",
    "print(\"\\nOverlap Information:\")\n",
    "if train_val_overlap:\n",
    "    print(f\"- Overlap between Training and Validation: {sorted(train_val_overlap)}\")\n",
    "else:\n",
    "    print(\"- No overlap between Training and Validation.\")\n",
    "\n",
    "if train_test_overlap:\n",
    "    print(f\"- Overlap between Training and Testing: {sorted(train_test_overlap)}\")\n",
    "else:\n",
    "    print(\"- No overlap between Training and Testing.\")\n",
    "\n",
    "if val_test_overlap:\n",
    "    print(f\"- Overlap between Validation and Testing: {sorted(val_test_overlap)}\")\n",
    "else:\n",
    "    print(\"- No overlap between Validation and Testing.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
