{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd67827-b801-4815-9171-4ec1fa4690f2",
   "metadata": {},
   "source": [
    "# 7200 images - 3 epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52023ae7-8fc7-4465-8152-a90ab9b60857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/huuthanhvy.nguyen001/.cache/huggingface/token\n",
      "Login successful\n",
      "Initializing GPTModel with model_name: gpt-4o\n",
      "Initializing llamaModel with adapter: /home/huuthanhvy.nguyen001/LLMP/EXP/my_finetuned_llama_7200_images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e11200fcecf40e59870689ca9e8d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing llamaModel with model_name: meta-llama/Llama-3.2-11B-Vision-Instruct\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "sys.path.append('../')\n",
    "import LLMP as L\n",
    "\n",
    "load_dotenv()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Import LLMP after ensuring the path is correct\n",
    "import LLMP as L\n",
    "\n",
    "# Hugging Face login using the token\n",
    "from huggingface_hub import login\n",
    "\n",
    "login('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')\n",
    "\n",
    "model_instances = {\n",
    "    \"gpt4o\": L.GPTModel(\"gpt-4o\"),\n",
    "    \"CustomLLaMA\": L.llamafinetuned(\"/home/huuthanhvy.nguyen001/LLMP/EXP/finetuned-1000-images\"),\n",
    "    \"LLaMA\": L.llama(\"meta-llama/Llama-3.2-11B-Vision-Instruct\"),\n",
    "    \"GeminiProVision\": L.GeminiProVision(),\n",
    "    \"Gemini1_5Flash\": L.Gemini1_5Flash()\n",
    "}\n",
    "\n",
    "# Run the evaluator\n",
    "e = L.Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f1ad2-ea3a-48d1-a3db-07f6bbd3e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define the query\n",
    "bestquery = \"\"\"\n",
    "What is the exact acute angle degree? Give your answer as a specific number.\n",
    "No extra explanation is required.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generate images\n",
    "images = [L.GPImage.figure1('angle') for i in range(55)]\n",
    "\n",
    "# Start time measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the evaluator\n",
    "result = e.run(images, bestquery, model_instances)\n",
    "\n",
    "# End time measurement\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "# Save results to JSON file\n",
    "e.save_results('RESULTS/EXP1_Angle.json')\n",
    "\n",
    "# Save elapsed time to a text file\n",
    "with open('RESULTS/EXP1_Angle_Time.txt', 'w') as f:\n",
    "    f.write(f\"Elapsed time for running the query: {elapsed_time} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206f1efe-80bc-4a05-b0fe-105dacdce23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt4o, Average MLAE: 4.308, Confidence: 0.042, Standard Deviation: 0.022\n",
      "Model: CustomLLaMA, Average MLAE: 1.757, Confidence: 0.025, Standard Deviation: 0.013\n",
      "Model: LLaMA, Average MLAE: 4.426, Confidence: 0.099, Standard Deviation: 0.051\n",
      "Model: GeminiProVision, Average MLAE: 4.346, Confidence: 0.034, Standard Deviation: 0.017\n",
      "Model: Gemini1_5Flash, Average MLAE: 4.365, Confidence: 0.098, Standard Deviation: 0.05\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open('RESULTS/EXP1_Angle.json', 'r') as f:\n",
    "    result = json.load(f)\n",
    "\n",
    "# List of model names\n",
    "models = ['gpt4o', 'CustomLLaMA', 'LLaMA', 'GeminiProVision', 'Gemini1_5Flash']\n",
    "\n",
    "# Extract and print 'average_mlae', 'confidence', and 'std' for each model, rounded to 2 decimal places\n",
    "for model in models:\n",
    "    avg_mlae = round(result[model]['average_mlae'], 3)\n",
    "    confidence = round(result[model]['confidence'], 3)\n",
    "    std_dev = round(result[model]['std'], 3)\n",
    "    print(f\"Model: {model}, Average MLAE: {avg_mlae}, Confidence: {confidence}, Standard Deviation: {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5a202-5119-4abe-827c-089b9476003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataframe = L.CreateDataFrame(result, model_instances, bestquery)\n",
    "\n",
    "df = create_dataframe.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305d7ed-5edd-4a42-a878-e40d0aecd027",
   "metadata": {},
   "source": [
    "# Comparision between ground truth and Parsed Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df5f05-1d6e-4b10-bd8f-76072aa42f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df[['Ground Truth', 'Parsed Answer']].plot(kind='line', ax=ax, marker='o')\n",
    "\n",
    "# Label each point\n",
    "for i in range(len(df)):\n",
    "    ax.text(i, df['Ground Truth'][i], f'{df[\"Ground Truth\"][i]}', ha='right', fontsize=9)\n",
    "    ax.text(i, df['Parsed Answer'][i], f'{df[\"Parsed Answer\"][i]}', ha='left', fontsize=9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246fa363-5b79-400e-aec6-d3213ed33dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 21 14:00:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              56W / 400W |  24202MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2069412      C   ...1/anaconda3/envs/pytorch/bin/python    24194MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!kill -9 2069412"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
